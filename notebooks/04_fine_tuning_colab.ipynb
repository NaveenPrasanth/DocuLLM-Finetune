{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocuMind: End-to-End Document Understanding Pipeline\n",
    "\n",
    "This notebook runs the **full DocuMind pipeline** on Google Colab:\n",
    "\n",
    "1. **Setup** — Clone repo, install dependencies, check GPU\n",
    "2. **Data** — Download CORD v2, prepare splits, visualize samples\n",
    "3. **Prompt Engineering** — Run 7 strategies on the base model\n",
    "4. **Fine-Tuning** — QLoRA fine-tune Qwen2-VL-2B on receipts\n",
    "5. **Evaluation** — Compare base vs prompted vs fine-tuned\n",
    "\n",
    "**Requirements:** Google Colab with a T4 GPU (free tier) or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ════════════════════════════════════════════════════════════════════\n# Cell 1: Clone repo and install dependencies\n# ════════════════════════════════════════════════════════════════════\nimport os\n\n# Reset to a known-good directory (handles re-runs after rm -rf)\ntry:\n    os.getcwd()\nexcept OSError:\n    os.chdir(\"/content\")\n\n# ── Clone the repo ─────────────────────────────────────────────────\nREPO_URL = \"https://github.com/NaveenPrasanth/DocuLLM-Finetune.git\"\nREPO_DIR = \"/content/DocuLLM-Finetune\"\n\nif not os.path.exists(REPO_DIR):\n    os.chdir(\"/content\")\n    !git clone {REPO_URL} {REPO_DIR}\n    print(f\"Cloned repo to {REPO_DIR}\")\nelse:\n    !cd {REPO_DIR} && git pull\n    print(f\"Repo already exists, pulled latest\")\n\nos.chdir(REPO_DIR)\nprint(f\"Working directory: {os.getcwd()}\")\n\n# ── Install dependencies ───────────────────────────────────────────\n!pip install -q \\\n    torch \\\n    \"transformers>=4.45.0\" \\\n    \"accelerate>=0.34.0\" \\\n    \"peft>=0.13.0\" \\\n    \"bitsandbytes>=0.43.0\" \\\n    \"qwen-vl-utils>=0.0.2\" \\\n    \"datasets>=2.20.0\" \\\n    \"omegaconf>=2.3\" \\\n    \"pydantic>=2.5\" \\\n    \"rapidfuzz>=3.5\" \\\n    \"python-dotenv>=1.0\" \\\n    wandb rich tqdm matplotlib seaborn Pillow\n\n# Install the project itself\n!pip install -q -e .\n\nprint(\"\\nAll dependencies installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Cell 2: Imports and GPU check\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports — this is the whole point of having a codebase!\n",
    "from src.config import load_base_config, load_training_config, load_data_config\n",
    "from src.data.cord_loader import load_cord_dataset, parse_cord_ground_truth, get_cord_schema\n",
    "from src.data.format_converter import sample_to_chatml, sample_to_inference_chatml\n",
    "from src.data.dataset_builder import build_cord_splits, get_dataset_stats\n",
    "from src.training.model_loader import load_quantized_model, load_processor, print_model_info\n",
    "from src.training.lora_config import build_lora_config, apply_lora\n",
    "\n",
    "# Seed everything\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# GPU check\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"No GPU detected! Go to Runtime > Change runtime type > GPU.\\n\"\n",
    "        \"A T4 (free tier) with 15GB VRAM is sufficient.\"\n",
    "    )\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "props = torch.cuda.get_device_properties(0)\n",
    "gpu_mem = getattr(props, 'total_memory', getattr(props, 'total_mem', 0)) / (1024**3)\n",
    "print(f\"GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "if gpu_mem < 14:\n",
    "    print(\"WARNING: <14GB VRAM — training may fail.\")\n",
    "else:\n",
    "    print(\"VRAM sufficient for QLoRA.\")\n",
    "\n",
    "# Load configs from YAML\n",
    "base_config = load_base_config()\n",
    "training_config = load_training_config()\n",
    "data_config = load_data_config(\"cord\")\n",
    "print(f\"\\nProject: {base_config.project.name}\")\n",
    "print(f\"Model: {base_config.model.name}\")\n",
    "print(f\"Dataset: {data_config.dataset.name} ({data_config.dataset.hf_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ════════════════════════════════════════════════════════════════════\n# Cell 3: Load and explore the CORD dataset\n# ════════════════════════════════════════════════════════════════════\nimport gc\n\n# ── Colab free-tier RAM budget ─────────────────────────────────────\n# Colab free tier has ~12.7GB system RAM. Loading 720 PIL images +\n# model weights + tokenizer easily exceeds this. We cap training\n# samples to keep RAM under control while still getting meaningful\n# fine-tuning results.\nMAX_TRAIN = 200   # 200 samples is plenty for QLoRA on a small domain\nMAX_VAL = 30\nMAX_TEST = 20\n\n# Use our data pipeline\nprint(\"Loading CORD v2 dataset...\")\nsplits = build_cord_splits(seed=SEED)\n\nfor split_name, samples in splits.items():\n    stats = get_dataset_stats(samples)\n    print(f\"  {split_name}: {stats['num_samples']} samples, \"\n          f\"avg {stats['avg_fields']:.1f} fields/sample \"\n          f\"(range: {stats['min_fields']}-{stats['max_fields']})\")\n\n# Cap splits to fit in Colab RAM\ntrain_samples = splits[\"train\"][:MAX_TRAIN]\nval_samples = splits[\"val\"][:MAX_VAL]\ntest_samples = splits[\"test\"][:MAX_TEST]\n\n# Free the full splits we no longer need\ndel splits\ngc.collect()\n\nprint(f\"\\nUsing: {len(train_samples)} train | {len(val_samples)} val | {len(test_samples)} test\")\nprint(\"(Capped for Colab free-tier 12.7GB RAM budget)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Cell 4: Visualize a training data point (input → output)\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "from src.data.format_converter import DEFAULT_INSTRUCTION\n",
    "\n",
    "sample_idx = 0\n",
    "sample = train_samples[sample_idx]\n",
    "\n",
    "fig, (ax_img, ax_txt) = plt.subplots(1, 2, figsize=(16, 8),\n",
    "                                      gridspec_kw={\"width_ratios\": [1, 1.2]})\n",
    "\n",
    "# Left: receipt image\n",
    "ax_img.imshow(sample[\"image\"])\n",
    "ax_img.set_title(\"INPUT: Receipt Image\", fontsize=14, fontweight=\"bold\")\n",
    "ax_img.axis(\"off\")\n",
    "\n",
    "# Right: expected JSON output\n",
    "gt_json = json.dumps(sample[\"ground_truth\"], indent=2, ensure_ascii=False)\n",
    "lines = gt_json.split(\"\\n\")\n",
    "display_text = \"\\n\".join(lines[:60])\n",
    "if len(lines) > 60:\n",
    "    display_text += \"\\n... (truncated)\"\n",
    "\n",
    "ax_txt.axis(\"off\")\n",
    "ax_txt.set_title(\"OUTPUT: Expected JSON\", fontsize=14, fontweight=\"bold\")\n",
    "ax_txt.text(0.02, 0.98, display_text, transform=ax_txt.transAxes,\n",
    "            fontsize=8, fontfamily=\"monospace\", verticalalignment=\"top\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"#f0f0f0\", alpha=0.9))\n",
    "\n",
    "plt.suptitle(f\"Training Sample #{sample_idx}\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print what the model actually sees\n",
    "print(f\"INSTRUCTION: {DEFAULT_INSTRUCTION}\")\n",
    "print(f\"\\nImage size: {sample['image'].size}\")\n",
    "print(f\"Top-level keys: {list(sample['ground_truth'].keys())}\")\n",
    "print(f\"Flat fields: {sample['metadata']['num_fields']}\")\n",
    "print(f\"JSON length: {len(gt_json)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Cell 5: Load Qwen2-VL with 4-bit quantization\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Use our config-driven model loader\n",
    "tc = training_config.training\n",
    "\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = load_quantized_model(\n",
    "    model_name=base_config.model.name,\n",
    "    quantization_config=tc.quantization,\n",
    ")\n",
    "\n",
    "processor = load_processor(\n",
    "    processor_name=base_config.model.processor_name,\n",
    ")\n",
    "\n",
    "print_model_info(model)\n",
    "\n",
    "allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "print(f\"\\nGPU memory after model load: {allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ════════════════════════════════════════════════════════════════════\n# Cell 6: Quick base model test (before any fine-tuning)\n# ════════════════════════════════════════════════════════════════════\nfrom qwen_vl_utils import process_vision_info\nfrom src.inference.postprocessor import postprocess_prediction\n\ndef run_inference(model, processor, image, instruction=None):\n    \"\"\"Run inference on a single image.\"\"\"\n    if instruction is None:\n        instruction = DEFAULT_INSTRUCTION\n\n    messages = [{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": image},\n            {\"type\": \"text\", \"text\": instruction},\n        ],\n    }]\n\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(text=[text], images=image_inputs, videos=video_inputs,\n                       padding=True, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    model.eval()\n    with torch.no_grad():\n        output_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=False)\n\n    generated_ids = output_ids[:, inputs[\"input_ids\"].shape[1]:]\n    return processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n\n# Test base model on 1 sample\ntest_item = test_samples[0]\nprint(\"Base model prediction (before fine-tuning):\")\nprint(\"=\" * 60)\nbase_prediction = run_inference(model, processor, test_item[\"image\"])\nprint(base_prediction[:500])\nprint(\"=\" * 60)\n\n# Use our postprocessor (handles markdown fences, fixes common JSON issues)\nbase_processed = postprocess_prediction(base_prediction)\nprint(f\"Valid JSON: {'Yes' if base_processed['valid'] else 'No'}\")\nif base_processed['errors']:\n    print(f\"Notes: {base_processed['errors']}\")\nif base_processed['valid']:\n    print(f\"Keys: {list(base_processed['parsed'].keys())}\")\n    print(\"\\nNote: Base model outputs free-form JSON, not the CORD schema.\")\n    print(\"Fine-tuning teaches the model the exact schema structure.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Cell 7: Apply LoRA adapters\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "from peft import LoraConfig as PeftLoraConfig\n",
    "\n",
    "# Use our config-driven LoRA setup\n",
    "lora_config = build_lora_config(tc.lora)\n",
    "model = apply_lora(model, lora_config)\n",
    "\n",
    "allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "print(f\"\\nGPU memory after LoRA: {allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ════════════════════════════════════════════════════════════════════\n# Cell 8: Prepare data and train\n# ════════════════════════════════════════════════════════════════════\nimport gc\nfrom datasets import Dataset\nfrom transformers import TrainingArguments, Trainer\nfrom src.data.format_converter import DEFAULT_INSTRUCTION\n\n# ── Optional W&B ──────────────────────────────────────────────────\nUSE_WANDB = False  # Set True and login first\nif USE_WANDB:\n    import wandb\n    wandb.init(project=\"documind\", tags=[\"qwen2-vl\", \"qlora\", \"cord\"])\n\n# ── Convert samples to ChatML training format ─────────────────────\nINSTRUCTION = DEFAULT_INSTRUCTION\n\ndef sample_to_training_dict(sample):\n    gt_json = json.dumps(sample[\"ground_truth\"], indent=2, ensure_ascii=False)\n    return {\n        \"messages\": sample_to_chatml(\n            image=sample[\"image\"],\n            ground_truth_json=gt_json,\n            instruction=INSTRUCTION,\n        )\n    }\n\nprint(\"Converting to ChatML format...\")\ntrain_data = [sample_to_training_dict(s) for s in train_samples]\nval_data = [sample_to_training_dict(s) for s in val_samples]\nprint(f\"Train: {len(train_data)} | Val: {len(val_data)}\")\n\n# Free raw samples — the PIL images are now inside train_data messages\ndel train_samples, val_samples\ngc.collect()\n\ntrain_dataset = Dataset.from_list(train_data)\neval_dataset = Dataset.from_list(val_data)\n\n# ── Custom collator for Qwen2-VL multimodal training ──────────────\ndef find_assistant_tokens(input_ids, tokenizer):\n    \"\"\"Find assistant response regions for label masking.\"\"\"\n    assistant_start = tokenizer.encode(\"assistant\\n\", add_special_tokens=False)\n    end_marker = tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)\n    ids = input_ids.tolist()\n    regions = []\n    i = 0\n    while i < len(ids) - len(assistant_start):\n        if ids[i:i+len(assistant_start)] == assistant_start:\n            start = i + len(assistant_start)\n            j = start\n            while j < len(ids) - len(end_marker) + 1:\n                if ids[j:j+len(end_marker)] == end_marker:\n                    regions.append((start, j + len(end_marker)))\n                    i = j + len(end_marker)\n                    break\n                j += 1\n            else:\n                regions.append((start, len(ids)))\n                break\n        i += 1\n    return regions\n\n\ndef collate_fn(examples):\n    \"\"\"Collator: chat template + vision processing + label masking.\"\"\"\n    texts, all_images = [], []\n    for ex in examples:\n        text = processor.apply_chat_template(\n            ex[\"messages\"], tokenize=False, add_generation_prompt=False\n        )\n        texts.append(text)\n        images, _ = process_vision_info(ex[\"messages\"])\n        if images:\n            all_images.extend(images)\n\n    batch = processor(\n        text=texts,\n        images=all_images if all_images else None,\n        padding=True, truncation=True, max_length=2048,\n        return_tensors=\"pt\",\n    )\n\n    # Label masking: only compute loss on assistant responses\n    labels = batch[\"input_ids\"].clone()\n    for i in range(len(examples)):\n        regions = find_assistant_tokens(batch[\"input_ids\"][i], processor.tokenizer)\n        labels[i, :] = -100\n        for start, end in regions:\n            labels[i, start:end] = batch[\"input_ids\"][i, start:end]\n    if processor.tokenizer.pad_token_id is not None:\n        labels[labels == processor.tokenizer.pad_token_id] = -100\n    batch[\"labels\"] = labels\n    return batch\n\n# ── Sanity check the collator ─────────────────────────────────────\nprint(\"\\nTesting collator...\")\ntest_batch = collate_fn([train_data[0]])\nn_labeled = (test_batch['labels'][0] != -100).sum().item()\nn_total = test_batch['input_ids'].shape[1]\nprint(f\"  input_ids: {test_batch['input_ids'].shape}\")\nprint(f\"  Labeled tokens: {n_labeled}/{n_total} ({100*n_labeled/n_total:.1f}%)\")\ndel test_batch\ngc.collect()\ntorch.cuda.empty_cache()\n\n# ── Training args ─────────────────────────────────────────────────\n# With 200 train samples and batch_size=1, grad_accum=4:\n#   steps_per_epoch = 200 / (1 * 4) = 50\n#   total_steps = 50 * 3 = 150\n#   warmup_steps = 15 (10% of 150)\nOUTPUT_DIR = \"./outputs/qlora_qwen2vl_cord\"\nGRAD_ACCUM = 4  # reduced from 8 since we have fewer samples\nNUM_EPOCHS = 3\nSTEPS_PER_EPOCH = len(train_dataset) // GRAD_ACCUM\nTOTAL_STEPS = STEPS_PER_EPOCH * NUM_EPOCHS\nWARMUP_STEPS = max(1, int(TOTAL_STEPS * 0.1))\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=GRAD_ACCUM,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=WARMUP_STEPS,\n    weight_decay=0.01,\n    bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n    logging_steps=5,\n    eval_strategy=\"steps\",\n    eval_steps=STEPS_PER_EPOCH,       # eval once per epoch\n    save_steps=STEPS_PER_EPOCH,       # save once per epoch\n    save_total_limit=2,\n    dataloader_num_workers=0,\n    remove_unused_columns=False,\n    report_to=\"wandb\" if USE_WANDB else \"none\",\n    logging_first_step=True,\n    optim=\"paged_adamw_8bit\",\n    dataloader_pin_memory=False,\n)\n\n# ── Train ─────────────────────────────────────────────────────────\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=collate_fn,\n)\n\nprint(f\"\\nStarting training...\")\nprint(f\"  {len(train_dataset)} samples, batch=1, grad_accum={GRAD_ACCUM}\")\nprint(f\"  {STEPS_PER_EPOCH} steps/epoch x {NUM_EPOCHS} epochs = {TOTAL_STEPS} total steps\")\nprint(f\"  Warmup: {WARMUP_STEPS} steps | Eval every {STEPS_PER_EPOCH} steps\")\nprint(f\"  Logging every 5 steps — first output in ~1 min\\n\")\n\ntrainer.train()\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Cell 9: Plot training curves\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_steps = [e[\"step\"] for e in log_history if \"loss\" in e and \"eval_loss\" not in e]\n",
    "train_losses = [e[\"loss\"] for e in log_history if \"loss\" in e and \"eval_loss\" not in e]\n",
    "eval_steps = [e[\"step\"] for e in log_history if \"eval_loss\" in e]\n",
    "eval_losses = [e[\"eval_loss\"] for e in log_history if \"eval_loss\" in e]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_steps, train_losses, \"b-\", lw=1.5, label=\"Train\")\n",
    "axes[0].set_xlabel(\"Step\"); axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training Loss\"); axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "\n",
    "if eval_losses:\n",
    "    axes[1].plot(eval_steps, eval_losses, \"r-o\", lw=1.5, label=\"Eval\")\n",
    "    axes[1].set_xlabel(\"Step\"); axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].set_title(\"Eval Loss\"); axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"No eval data yet\", ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if train_losses:\n",
    "    print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "if eval_losses:\n",
    "    print(f\"Final eval loss:  {eval_losses[-1]:.4f}\")\n",
    "    print(f\"Best eval loss:   {min(eval_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Cell 10: Evaluate — Base vs Fine-Tuned side by side\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "from src.data.cord_loader import flatten_cord_fields\n",
    "from src.inference.postprocessor import postprocess_prediction\n",
    "from src.evaluation.metrics import compute_field_f1, compute_json_validity\n",
    "\n",
    "NUM_EVAL = 10\n",
    "eval_samples = test_samples[:NUM_EVAL]\n",
    "\n",
    "ft_results = {\"f1\": [], \"json_valid\": []}\n",
    "\n",
    "print(f\"Evaluating fine-tuned model on {NUM_EVAL} test samples...\\n\")\n",
    "\n",
    "for i, sample in enumerate(eval_samples):\n",
    "    prediction = run_inference(model, processor, sample[\"image\"])\n",
    "    processed = postprocess_prediction(prediction)\n",
    "\n",
    "    # JSON validity\n",
    "    ft_results[\"json_valid\"].append(1.0 if processed[\"valid\"] else 0.0)\n",
    "\n",
    "    # Field F1\n",
    "    if processed[\"valid\"]:\n",
    "        pred_flat = flatten_cord_fields(processed[\"parsed\"])\n",
    "        gt_flat = sample[\"ground_truth_flat\"]\n",
    "        f1_result = compute_field_f1(pred_flat, gt_flat)\n",
    "        ft_results[\"f1\"].append(f1_result[\"micro\"][\"f1\"])\n",
    "    else:\n",
    "        ft_results[\"f1\"].append(0.0)\n",
    "\n",
    "    if i < 3:  # Show first 3\n",
    "        print(f\"── Sample {i} ──\")\n",
    "        print(f\"  Valid JSON: {processed['valid']}\")\n",
    "        print(f\"  Field F1:   {ft_results['f1'][-1]:.3f}\")\n",
    "        print(f\"  Prediction: {prediction[:200]}...\\n\")\n",
    "\n",
    "avg_f1 = sum(ft_results['f1']) / len(ft_results['f1'])\n",
    "avg_valid = sum(ft_results['json_valid']) / len(ft_results['json_valid'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Fine-Tuned Model Results ({NUM_EVAL} samples)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Avg Field F1:     {avg_f1:.4f}\")\n",
    "print(f\"  JSON Valid Rate:  {avg_valid:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Cell 11: Save adapter to Google Drive\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "DRIVE_SAVE_DIR = \"/content/drive/MyDrive/documind/adapters/qlora_qwen2vl_cord\"\n",
    "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving adapter to {DRIVE_SAVE_DIR}...\")\n",
    "model.save_pretrained(DRIVE_SAVE_DIR)\n",
    "processor.save_pretrained(DRIVE_SAVE_DIR)\n",
    "\n",
    "# Save training config + results\n",
    "config_to_save = {\n",
    "    \"model_name\": base_config.model.name,\n",
    "    \"lora\": tc.lora.model_dump(),\n",
    "    \"training_args\": {\n",
    "        \"epochs\": tc.args.num_train_epochs,\n",
    "        \"lr\": tc.args.learning_rate,\n",
    "        \"batch_size\": tc.args.per_device_train_batch_size,\n",
    "        \"grad_accum\": tc.args.gradient_accumulation_steps,\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"avg_field_f1\": avg_f1,\n",
    "        \"json_valid_rate\": avg_valid,\n",
    "        \"final_train_loss\": train_losses[-1] if train_losses else None,\n",
    "        \"final_eval_loss\": eval_losses[-1] if eval_losses else None,\n",
    "    },\n",
    "    \"dataset\": {\"train\": len(train_data), \"val\": len(val_data), \"test_eval\": NUM_EVAL},\n",
    "}\n",
    "with open(f\"{DRIVE_SAVE_DIR}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_to_save, f, indent=2)\n",
    "\n",
    "saved_files = os.listdir(DRIVE_SAVE_DIR)\n",
    "print(f\"\\nSaved {len(saved_files)} files:\")\n",
    "for fn in sorted(saved_files):\n",
    "    size = os.path.getsize(os.path.join(DRIVE_SAVE_DIR, fn))\n",
    "    print(f\"  {fn:40s} {size / 1024:.1f} KB\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    if wandb.run:\n",
    "        wandb.finish()\n",
    "\n",
    "print(\"\\nDone! Adapter saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}