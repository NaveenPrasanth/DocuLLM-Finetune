{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# 02 - Prompt Engineering Experiments\n",
    "\n",
    "Evaluate 7 prompt strategies on the Qwen2-VL base model for CORD receipt extraction.\n",
    "\n",
    "**Strategies tested:**\n",
    "1. Zero-shot basic - minimal instruction\n",
    "2. Zero-shot detailed - field categories described\n",
    "3. Zero-shot structured - explicit JSON schema\n",
    "4. Few-shot (2 examples) - two representative examples\n",
    "5. Few-shot (5 examples) - five diverse examples\n",
    "6. Chain-of-thought step-by-step - guided reasoning\n",
    "7. Chain-of-thought self-verify - extract then verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": "# ── Setup: works on both Colab and local ──────────────────────────\nimport os, sys\n\nIN_COLAB = 'google.colab' in sys.modules or os.path.exists('/content')\n\nif IN_COLAB:\n    REPO_URL = \"https://github.com/NaveenPrasanth/DocuLLM-Finetune.git\"\n    REPO_DIR = \"/content/DocuLLM-Finetune\"\n    if not os.path.exists(REPO_DIR):\n        !git clone {REPO_URL} {REPO_DIR}\n    os.chdir(REPO_DIR)\n    !pip install -q torch \"transformers>=4.45.0\" \"accelerate>=0.34.0\" \\\n        \"qwen-vl-utils>=0.0.2\" \"datasets>=2.20.0\" \"omegaconf>=2.3\" \\\n        \"pydantic>=2.5\" \"rapidfuzz>=3.5\" \"python-dotenv>=1.0\" rich Pillow\n    !pip install -q -e .\nelse:\n    sys.path.insert(0, '..')\n\nimport json\nimport torch\nfrom pathlib import Path\nfrom rich.console import Console\nfrom rich.table import Table\n\nfrom src.config import load_base_config\nfrom src.data.cord_loader import load_cord_dataset, get_cord_schema\nfrom src.data.format_converter import sample_to_inference_chatml\nfrom src.prompts.templates import ReceiptExtractionTemplate, get_template, list_templates\nfrom src.prompts.strategies import (\n    ZeroShotBasic,\n    ZeroShotDetailed,\n    ZeroShotStructured,\n    FewShotStrategy2,\n    FewShotStrategy5,\n    ChainOfThoughtStepByStep,\n    ChainOfThoughtSelfVerify,\n    get_all_strategies,\n    list_strategies,\n)\nfrom src.prompts.experiment_runner import (\n    PromptExperimentRunner,\n    extract_json_from_response,\n)\n\nconsole = Console()\nconfig = load_base_config()\nprint(f'Project: {config.project.name}')\nprint(f'Model:   {config.model.name}')\nprint(f'Available strategies: {list_strategies()}')\nprint(f'Available templates:  {list_templates()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "model_name = config.model.name\n",
    "print(f'Loading model: {model_name}')\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(f'Model loaded on device: {device}')\n",
    "print(f'Model dtype: {next(model.parameters()).dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-define-strategies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test samples and few-shot examples\n",
    "test_samples = load_cord_dataset(split='test', max_samples=5)\n",
    "example_samples = load_cord_dataset(split='train', max_samples=10)\n",
    "\n",
    "print(f'Test samples:    {len(test_samples)}')\n",
    "print(f'Example samples: {len(example_samples)}')\n",
    "\n",
    "# Define all 7 strategies\n",
    "strategies = [\n",
    "    ZeroShotBasic(),\n",
    "    ZeroShotDetailed(),\n",
    "    ZeroShotStructured(),\n",
    "    FewShotStrategy2(example_samples=example_samples),\n",
    "    FewShotStrategy5(example_samples=example_samples),\n",
    "    ChainOfThoughtStepByStep(),\n",
    "    ChainOfThoughtSelfVerify(),\n",
    "]\n",
    "\n",
    "print(f'\\nDefined {len(strategies)} strategies:')\n",
    "for s in strategies:\n",
    "    print(f'  - {s.get_name()}')\n",
    "\n",
    "# Preview the prompt for the first strategy on the first sample\n",
    "print(f'\\n--- Sample prompt for \"{strategies[0].get_name()}\" ---')\n",
    "print(strategies[0].build_prompt(test_samples[0]))\n",
    "print('\\n--- Sample prompt for \"zero_shot_structured\" ---')\n",
    "print(strategies[2].build_prompt(test_samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each strategy on 5 test samples\n",
    "runner = PromptExperimentRunner(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    strategies=strategies,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "results = runner.run_experiment(\n",
    "    test_samples=test_samples,\n",
    "    num_samples=5,\n",
    ")\n",
    "\n",
    "# Show raw predictions for the first sample across all strategies\n",
    "print('=== Predictions for first test sample ===')\n",
    "print(f'Ground truth: {json.dumps(test_samples[0][\"ground_truth\"], indent=2, ensure_ascii=False)[:500]}')\n",
    "print()\n",
    "\n",
    "for strat_result in results['strategy_results']:\n",
    "    name = strat_result['strategy']\n",
    "    first = strat_result['per_sample'][0]\n",
    "    print(f'--- {name} ---')\n",
    "    print(f'JSON Valid: {first[\"metrics\"][\"json_valid\"]}')\n",
    "    print(f'F1: {first[\"metrics\"][\"f1\"]:.4f}')\n",
    "    output_preview = first['raw_output'][:300]\n",
    "    print(f'Output (preview): {output_preview}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-results",
   "metadata": {},
   "outputs": [],
   "source": "# Build comparison table\ntable = Table(\n    title='Prompt Strategy Comparison (5 samples)',\n    show_header=True,\n    header_style='bold magenta',\n)\ntable.add_column('Rank', style='dim', width=4, justify='right')\ntable.add_column('Strategy', style='cyan', min_width=22)\ntable.add_column('JSON Valid', justify='right')\ntable.add_column('Avg F1', justify='right')\ntable.add_column('Micro F1', justify='right')\ntable.add_column('Precision', justify='right')\ntable.add_column('Recall', justify='right')\ntable.add_column('Time (s)', justify='right')\n\nfor i, row in enumerate(results['comparison']):\n    style = 'bold green' if i == 0 else None\n    table.add_row(\n        str(i + 1),\n        row['strategy'],\n        f\"{row['json_valid_rate']:.0%}\",\n        f\"{row['avg_f1']:.4f}\",\n        f\"{row['micro_f1']:.4f}\",\n        f\"{row['avg_precision']:.4f}\",\n        f\"{row['avg_recall']:.4f}\",\n        f\"{row['elapsed_seconds']:.1f}\",\n        style=style,\n    )\n\nconsole.print(table)\n\n# Save results\noutput_path = Path('outputs/prompt_experiments/notebook_results.json')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\nPromptExperimentRunner.save_results(results, output_path)\nprint(f'\\nResults saved to: {output_path}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-analysis",
   "metadata": {},
   "source": [
    "## Analysis and Findings\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "| Dimension | Finding |\n",
    "|-----------|----------|\n",
    "| **JSON Validity** | _[Fill in: Which strategies most reliably produce valid JSON?]_ |\n",
    "| **Field Accuracy** | _[Fill in: Which strategy achieves the highest field-level F1?]_ |\n",
    "| **Schema Guidance** | _[Fill in: How much does providing the JSON schema help?]_ |\n",
    "| **Few-shot Scaling** | _[Fill in: Does 5-shot outperform 2-shot? By how much?]_ |\n",
    "| **Chain-of-Thought** | _[Fill in: Does CoT reasoning improve extraction quality?]_ |\n",
    "| **Self-Verification** | _[Fill in: Does the verify step catch and correct errors?]_ |\n",
    "| **Latency** | _[Fill in: What is the latency cost of more complex prompts?]_ |\n",
    "\n",
    "### Hypotheses for Next Steps\n",
    "\n",
    "1. **Best base prompt for fine-tuning**: The strategy with the highest F1 should be used as the instruction during QLoRA fine-tuning.\n",
    "2. **Schema guidance is likely critical**: Structured prompts should significantly outperform basic ones, confirming the need for schema in the training prompt.\n",
    "3. **CoT may not help base model**: Without fine-tuning, the base model may not reliably follow multi-step reasoning, suggesting CoT benefits may only appear post-training.\n",
    "4. **Few-shot examples help format compliance**: Even if field accuracy is similar, few-shot examples should improve JSON validity rates.\n",
    "\n",
    "### Recommended Strategy for Phase 4 (Fine-tuning)\n",
    "\n",
    "_[Fill in: Which strategy to use as the training instruction and why]_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}